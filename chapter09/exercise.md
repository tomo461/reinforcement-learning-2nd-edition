# 第9章 演習問題

# Exercise 9.1

本書の第I部で示されているようなテーブル形式の手法が、線形関数近似の特殊なケースであることを示せ。
特徴ベクトルはテーブル形式の手法では何に対応するか？

## 回答

線形関数近似では、状態 $s$ の近似価値関数 $\hat{v}(s,\mathbf{w})$ は、特徴ベクトル $\mathbf{x}(s)$ と重みベクトル $\mathbf{w}$ の内積として定義される。

$$
\hat{v}(s,\mathbf{w}) = \mathbf{w}^{\top} \mathbf{x}(s) = \sum_{i=1}^{d} w_{i} x_{i}(s)
$$

ここで、 $\mathbf{w} \in \mathbb{R}^{d}$ は学習可能な重みベクトルであり、 $d$ は重みの次元（特徴の数）である。

テーブル形式の手法では、各状態 $s$ に対して独立した値が割り当てられ、これらの値はテーブルとして保存される。つまり、状態 $s$ に対応する価値は、テーブルのエントリ $V(s)$ として表される。

このテーブル形式の手法を線形関数近似の枠組みで表現するために、特徴ベクトル $\mathbf{x}(s)$ を次のように定義する。

$$
x_{i}(s) =
\begin{cases}
1 & \text{if } \quad i = s \\
0 & \text{otherwise}
\end{cases}
$$

ここで、 $d$ は状態空間のサイズ $|\mathcal{S}|$ に等しいとする。\
この特徴ベクトル $\mathbf{x}(s)$ は状態 $s$ に対応するインデックスのみが1であり、他のすべてのインデックスは0である単位ベクトルである。
この特徴ベクトルを用いると、線形関数近似における価値関数の近似は次のようになる。

$$
\hat{v}(s,\mathbf{w}) = \sum_{i=1}^{d} w_{i} x_{i}(s) = w_{s} \cdot 1 + \sum_{i \neq s} w_{i} \cdot 0 = w_{s} = V(s)
$$

したがって、 $\hat{v}(s,\mathbf{w})$ はテーブル形式の手法での価値 $V(s)$ と等しくなる。\
つまり、テーブル形式の手法は、線形関数近似の特殊なケースであり、特徴ベクトルは状態に対応する単位ベクトルとして表されることが示された。


# Exercise 9.2

なぜ以下の式(9.17) は次元数 $k$ に関して $(n+1)^k$ 個の異なる特徴量を定義するのか。

$$
x_i(s) = \prod_{j=1}^{k} s_j^{c_{i,j}} \quad \text{where} \quad c_{i,j} \in \{0, 1, \ldots, n\}
$$

## 回答

この式は、多項式特徴量を生成するためのものであり、各特徴量 $x_i(s)$ は状態ベクトル $s = (s_1, s_2, \ldots, s_k)$ の各成分 $s_j$ に対して異なる指数 $c_{i,j}$ を持つ積として定義されている。
各成分 $s_j$ に対して、指数 $c_{i,j}$ は $0$ から $n$ までの $(n+1)$ 通りの値を取る。\
したがって、各成分 $s_j$ に対して $(n+1)$ 通りの選択肢があることになる。


# Exercise 9.3

特徴ベクトルが $\mathbf{x}(s) = (1, s_1, s_2, s_1 s_2, s_1^2, s_2^2, s_1 s_2^2, s_1^2 s_2, s_1^2 s_2^2)$ になるのは、どんな $n$ と $c_{i,j}$ の場合か。

## 回答

この特徴ベクトルは、2次元の状態空間 $s = (s_1, s_2)$ に対して定義されており、各成分 $s_j$ に対して指数 $c_{i,j}$ が $0$ から $2$ までの値を取る場合に対応する。\
具体的には、 $n = 2$ とし、各特徴量 $x_i(s)$ に対して次のように指数 $c_{i,j}$ を設定する。

- $x_1(s) = 1$ : $c_{1,1} = 0$, $c_{1,2} = 0$
- $x_2(s) = s_1$ : $c_{2,1} = 1$, $c_{2,2} = 0$
- $x_3(s) = s_2$ : $c_{3,1} = 0$, $c_{3,2} = 1$
- $x_4(s) = s_1 s_2$ : $c_{4,1} = 1$, $c_{4,2} = 1$
- $x_5(s) = s_1^2$ : $c_{5,1} = 2$, $c_{5,2} = 0$
- $x_6(s) = s_2^2$ : $c_{6,1} = 0$, $c_{6,2} = 2$
- $x_7(s) = s_1 s_2^2$ : $c_{7,1} = 1$, $c_{7,2} = 2$
- $x_8(s) = s_1^2 s_2$ : $c_{8,1} = 2$, $c_{8,2} = 1$
- $x_9(s) = s_1^2 s_2^2$ : $c_{9,1} = 2$, $c_{9,2} = 2$

したがって、 $n = 2$ の場合に、特徴ベクトル $\mathbf{x}(s) = (1, s_1, s_2, s_1 s_2, s_1^2, s_2^2, s_1 s_2^2, s_1^2 s_2, s_1^2 s_2^2)$ が得られる。


# Exercise 9.4

二つの状態に関する次元のうち、一方が他方よりも価値関数に影響を与える可能性が高いと考えて、繁華は主にこの次元に沿ってではなく、この次元全体に対して行われるべきだと思っていると仮定する。
この事前知識を活用するために、どのようなタイル化が利用できるか。

## 回答

重要度の高い次元ではなく、その次元をまたいで（つまり、重要度の低い次元に沿って）汎化を起こすには、重要度の低い次元に沿って受容野が細長く伸びるようなタイル化を利用することが考えられる。


# Exercise 9.5

タイルコーディングを使って7次元の連続的な状態空間を2値特徴ベクトルに変換し、状態価値関数  $\hat v(s, \mathbf w) = v_{\pi}(s) $ を推定するとする。
次元間に強い相互作用がないと考える、各次元に対して個別に8枚のタイリング（ストライプタイリング）を使用し、7×8=56 枚のタイリングをする。
加えて、次元間の二者間の相互作用がある可能性を考慮し、すべての二次元の組み合わせ $\binom{7}{2}$ = 21 通りを取り上げ、それぞれのペアを矩形タイルでタイリングする。各次元ペアに対して2枚のタイリングを作成し、合計で 21 × 2 + 56 = 98 枚のタイリングを用いることとする。

これらの特徴ベクトルを使用する際、いくらかのノイズを平均化する必要があると考え、学習を徐々に進めること、すなわち、学習が漸近線に近づくまでに、同じ特徴ベクトルを約10回提示することを望むとする。
このとき、ステップサイズパラメータ $\alpha$ として、どのような値を使用すべきか。それはなぜか。

## 回答

7次元の各次元のタイリングについては、1つの状態に対してアクティブになるタイルは各次元において1つ。つまり１つの状態に対して7個の特徴量がアクティブになる。

２次元の組み合わせの矩形タイリングについては、1つの状態に対してアクティブになるタイルは各ペアにおいて1つ。つまり１つの状態に対して21個の特徴量がアクティブになる。

したがって、1つの状態に対してアクティブになる特徴量の総数は、 $7 + 21 = 28$ 個である。

アクティブな特徴数 $n$ が常に一定である場合、 $\mathbb{E[\mathbf{x}^T \mathbf{x}]} = n = 28$ となる。また $\tau = 10$ であるから、式(9.19)からステップサイズパラメータ $\alpha$ を計算すると

$$
\begin{align*}
\alpha &= \big(\tau \mathbb{E[\mathbf{x}^T \mathbf{x}]}\big)^{-1} \\
&= \big(10 \times 28\big)^{-1} \\
&= \frac{1}{280}
\end{align*}
$$

したがって、ステップサイズパラメータ $\alpha$ として $\frac{1}{280}$ を使用することが適切である。
