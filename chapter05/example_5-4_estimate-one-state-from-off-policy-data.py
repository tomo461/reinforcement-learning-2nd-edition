"""Off-policy Monte Carlo estimation of a single blackjack state (Example 5.4).

This is a faithful Python rewrite of ``example_5-4_one-state-estimation-from-``
``off-policy-data.lisp`` from Sutton & Barto (2nd edition).  It estimates the
value of the state (dealer showing 2, player count 13 with a usable ace) under a
fixed target policy (stick on 20 or 21) using episodes generated by a behaviour
policy that hits or sticks with equal probability at every step.
"""

from __future__ import annotations

import random
from types import ModuleType
from typing import List, Optional, Sequence, Tuple

import numpy as np

HIT = 1
STICK = 0
BEHAVIOUR_PROB = 0.5  # behaviour policy chooses hit/stick uniformly
# state to be evaluated
TARGET_DEALER = 2
TARGET_PLAYER_TOTAL = 13
TARGET_USABLE_ACE = True

State = Tuple[int, int, bool]  # (dealer showing: dc, player count: pc, usable ace: ace)
EpisodeStep = Tuple[State, int]  # (state, action)
Episode = Tuple[int, List[EpisodeStep]]  # (reward, trajectory)

policy: Optional[np.ndarray] = None  # Target policy array

# Globals representing the evolving player hand (mirroring the Lisp code)
dc: int = TARGET_DEALER
pc: int = TARGET_PLAYER_TOTAL
ace: bool = TARGET_USABLE_ACE

RECORDED_EPISODES: List[Episode] = []


def _require_policy() -> np.ndarray:
    if policy is None:
        raise RuntimeError("Call setup() before generating episodes.")
    return policy


def card(rng: Optional[random.Random | ModuleType] = None) -> int:
    rng = rng or random
    return min(10, rng.randint(1, 13))


def setup(seed: Optional[int] = None) -> None:
    """Initialise the deterministic target policy array."""
    global policy, dc, pc, ace, RECORDED_EPISODES

    if seed is not None:
        random.seed(seed)

    policy = np.ones((11, 22, 2), dtype=int)
    policy[:, 20:, :] = STICK  # stick on 20 or 21

    dc = TARGET_DEALER
    pc = TARGET_PLAYER_TOTAL
    ace = TARGET_USABLE_ACE
    RECORDED_EPISODES = []


def reset_start_state() -> None:
    global dc, pc, ace
    dc = TARGET_DEALER
    pc = TARGET_PLAYER_TOTAL
    ace = TARGET_USABLE_ACE


def behaviour_action(rng: Optional[random.Random | ModuleType] = None) -> int:
    rng = rng or random
    return HIT if rng.random() < BEHAVIOUR_PROB else STICK


def generate_behaviour_episode(
    rng: Optional[random.Random | ModuleType] = None,
) -> Episode:
    """Generate one behaviour-policy episode from the fixed start state."""
    rng = rng or random
    _require_policy()

    reset_start_state()
    dc_hidden = card(rng)
    trajectory: List[EpisodeStep] = []

    while True:
        state: State = (dc, pc, ace)
        action = behaviour_action(rng)
        trajectory.append((state, action))

        if action == STICK:
            break

        draw_card(rng)
        if bust():
            break

    reward = outcome(dc, dc_hidden, rng)
    return reward, trajectory


def draw_card(rng: Optional[random.Random | ModuleType] = None) -> None:
    global pc, ace

    rng = rng or random
    card_value = card(rng)
    pc += card_value

    if not ace and card_value == 1:
        pc += 10
        ace = True

    if ace and pc > 21:
        pc -= 10
        ace = False


def bust() -> bool:
    return pc > 21


def outcome(
    dealer_showing: int,
    dealer_hidden: int,
    rng: Optional[random.Random | ModuleType] = None,
) -> int:
    """Play out the dealer hand and determine the final reward."""
    rng = rng or random

    dace = dealer_showing == 1 or dealer_hidden == 1
    dealer_total = dealer_showing + dealer_hidden + (10 if dace else 0)
    dealer_natural = dealer_total == 21
    player_natural = False  # starting count is 13, so this can never be True

    if player_natural and dealer_natural:
        return 0
    if player_natural:
        return 1
    if dealer_natural:
        return -1
    if bust():
        return -1

    while dealer_total < 17:
        next_card = card(rng)
        dealer_total += next_card

        if not dace and next_card == 1:
            dealer_total += 10
            dace = True

        if dace and dealer_total > 21:
            dealer_total -= 10
            dace = False

    if dealer_total > 21:
        return 1
    if dealer_total > pc:
        return -1
    if dealer_total == pc:
        return 0
    return 1


def importance_ratio(trajectory: Sequence[EpisodeStep]) -> float:
    """Return the product of target/behaviour probabilities for the episode."""
    target_policy = _require_policy()
    ratio = 1.0

    for (dealer_card, player_total, has_ace), action in trajectory:
        ace_idx = 1 if has_ace else 0
        target_action = int(target_policy[dealer_card, player_total, ace_idx])

        if action != target_action:
            return 0.0

        ratio *= 1.0 / BEHAVIOUR_PROB

    return ratio


def record_behaviour_episodes(
    num_episodes: int,
    rng: Optional[random.Random | ModuleType] = None,
) -> List[Episode]:
    """Generate and store behaviour episodes for later evaluation."""
    rng = rng or random
    episodes: List[Episode] = []

    for _ in range(num_episodes):
        episode = generate_behaviour_episode(rng)
        episodes.append(episode)

    RECORDED_EPISODES.extend(episodes)
    return episodes


def off_policy_estimate(
    num_episodes: int,
    episodes: Optional[Sequence[Episode]] = None,
    weighted: bool = False,  # use weighted importance sampling if True
    rng: Optional[random.Random | ModuleType] = None,
) -> List[float]:
    """Compute running off-policy estimates using the supplied episodes."""
    rng = rng or random

    if episodes is None:
        episodes = record_behaviour_episodes(num_episodes, rng)
    elif len(episodes) < num_episodes:
        raise ValueError("Not enough episodes provided for evaluation.")

    estimate = 0.0
    cumulative_weight = 0.0
    estimates: List[float] = []

    for idx in range(num_episodes):
        reward, trajectory = episodes[idx]
        rho = importance_ratio(trajectory)

        if weighted:
            # Weighted importance sampling
            # V_n+1 = V_n + (rho / C_n) * [G - V_n], where C_n = sum of weights
            cumulative_weight += rho
            if cumulative_weight > 0.0:
                estimate += (rho / cumulative_weight) * (reward - estimate)
        else:
            # Ordinary importance sampling
            # V_n+1 = V_n + (1/(n+1)) * [rho*G - V_n]
            estimate += (1.0 / (idx + 1)) * (rho * reward - estimate)

        estimates.append(estimate)

    return estimates


def main() -> None:
    rng = random.Random(2025)
    setup(seed=2025)

    ordinary_estimates = off_policy_estimate(100000, rng=rng)
    weighted_estimates = off_policy_estimate(100000, weighted=True, rng=rng)

    print(" ordinary_estimates[:100] =", ordinary_estimates[:100])
    print(" weighted_estimates[:100] =", weighted_estimates[:100])
    print(f"Ordinary IS estimate after 10,000 episodes: {ordinary_estimates[-1]:.5f}")
    print(f"Weighted IS estimate after 10,000 episodes: {weighted_estimates[-1]:.5f}")


if __name__ == "__main__":
    main()
