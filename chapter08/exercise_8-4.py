import matplotlib.pyplot as plt
import numpy as np


# ============================================================
# Shortcut Maze (Fully Matches Figure 8.5)
# ============================================================
class ShortcutMaze:
    def __init__(self, open_step=1000):
        # Grid size
        self.h = 6  # rows
        self.w = 9  # columns

        # Start (S) and Goal (G)
        self.start = (5, 3)  # (row, col)
        self.goal = (0, 8)  # (row, col)

        self.open_step = open_step
        self.global_step = 0

        # Initial barrier (horizontal wall)
        # Row=3, Col=0..7 are walls initially
        self.initial_walls = {(3, c) for c in range(0, self.w - 1)}
        self.opened_walls = set()
        self.closed_walls = set()
        self.reset_walls()

    def reset_walls(self):
        self.walls = set(self.initial_walls) - self.opened_walls
        self.walls.update(self.closed_walls)
        self.print_grid()

    def print_grid(self):
        """Print the current grid world state."""
        for r in range(self.h):
            row = []
            for c in range(self.w):
                if (r, c) == self.start:
                    row.append("S")
                elif (r, c) == self.goal:
                    row.append("G")
                elif (r, c) in self.walls:
                    row.append("#")
                else:
                    row.append(".")
            print(" ".join(row))
        print()

    def open_shortcut(self):
        # Open the right two tiles in the wall
        self.opened_walls = {(3, 0)}
        self.closed_walls = {(3, self.w - 1)}
        self.reset_walls()

    def reset(self):
        self.current = self.start
        return self.current

    def step(self, action):
        # Up=0, Down=1, Left=2, Right=3
        r, c = self.current

        if action == 0:
            nr, nc = r, c - 1
        elif action == 1:
            nr, nc = r, c + 1
        elif action == 2:
            nr, nc = r - 1, c
        else:
            nr, nc = r + 1, c

        # Environment changes on global time
        self.global_step += 1
        if self.global_step == self.open_step:
            self.open_shortcut()

        # Check bounds
        if not (0 <= nr < self.h):
            nr = r
        if not (0 <= nc < self.w):
            nc = c

        # Check walls
        if (nr, nc) in self.walls:
            nr, nc = r, c

        self.current = (nr, nc)
        reward = 1 if self.current == self.goal else 0
        return self.current, reward, reward > 0


# ============================================================
# Planning update
# ============================================================
def planning_update(Q, model, tau, alpha, gamma, kappa, use_bonus_in_update):
    # model keys are list of ((r,c), a)
    keys = list(model.keys())
    n_keys = len(keys)

    for _ in range(50):
        if n_keys == 0:
            return

        # Randomly select a state-action pair
        idx = np.random.randint(n_keys)
        (s, a) = keys[idx]

        s2, r = model[(s, a)]

        r_idx, c_idx = s

        if use_bonus_in_update:
            bonus = kappa * np.sqrt(tau[r_idx, c_idx, a])
            r = r + bonus

        r2_idx, c2_idx = s2

        # Q-learning update
        best_next_q = np.max(Q[r2_idx, c2_idx])
        Q[r_idx, c_idx, a] += alpha * (r + gamma * best_next_q - Q[r_idx, c_idx, a])

    return Q


# ============================================================
# Run a single experiment
# ============================================================
def run_experiment(use_bonus_in_update):
    env = ShortcutMaze()

    alpha = 0.1
    gamma = 0.95
    epsilon = 0.1
    kappa = 0.01

    # Initialize Q and tau as numpy arrays
    Q = np.zeros((env.h, env.w, 4))
    tau = np.zeros((env.h, env.w, 4))

    # Initialize model with all state-action pairs
    # Dyna-Q+ assumes unvisited pairs lead to same state with 0 reward
    model = dict()
    for r in range(env.h):
        for c in range(env.w):
            for a in range(4):
                model[((r, c), a)] = ((r, c), 0)

    rewards = []
    # Run for a fixed number of steps to match the figure style usually
    # But original code used episodes. Let's stick to episodes but be mindful of total steps.
    # Actually, to match Figure 8.5 (3000 steps or 6000 steps),
    # let's just run until we have enough steps?
    # The original code had `for ep in range(1000):`.
    # If the agent learns quickly, 1000 episodes is fine.

    # To prevent infinite loops if the agent gets stuck, we might want a step limit per episode,
    # but the original code didn't have one.

    steps_count = 0
    max_steps = 6000  # Figure 8.5 usually goes up to 6000 steps

    # We'll use a while loop on steps to be closer to the book's experiment style if we want,
    # but let's stick to the user's structure "for ep in range(1000)" to minimize changes,
    # UNLESS the user specifically asked to fix the logic which implies the result is wrong.
    # The result being wrong might be due to the logic bugs I found.

    for ep in range(1000):
        s = env.reset()

        while True:
            # Compute Q + bonus for selection
            r_idx, c_idx = s
            qb = Q[r_idx, c_idx].copy()

            if not use_bonus_in_update:
                # Action-selection bonus only
                qb += kappa * np.sqrt(tau[r_idx, c_idx])

            if np.random.rand() < epsilon:
                a = np.random.choice(4)
            else:
                a = np.argmax(qb)

            s2, r, done = env.step(a)
            rewards.append(r)
            steps_count += 1

            # tau update: increment ALL, then reset current
            tau += 1
            tau[r_idx, c_idx, a] = 0

            # Model update
            model[(s, a)] = (s2, r)

            # Direct RL update (no bonus here)
            r2_idx, c2_idx = s2
            best_next_q = np.max(Q[r2_idx, c2_idx])
            Q[r_idx, c_idx, a] += alpha * (r + gamma * best_next_q - Q[r_idx, c_idx, a])

            # Planning
            Q = planning_update(
                Q,
                model,
                tau,
                alpha,
                gamma,
                kappa,
                use_bonus_in_update=use_bonus_in_update,
            )

            s = s2
            if done:
                break

        if steps_count >= max_steps:
            break

    return np.array(rewards[:max_steps])


# ============================================================
# Run both methods
# ============================================================
np.random.seed(1234)
r_standard = run_experiment(use_bonus_in_update=True)
r_action_only = run_experiment(use_bonus_in_update=False)


def cumsum(x):
    return np.cumsum(x)


plt.figure(figsize=(12, 6))
plt.plot(cumsum(r_standard), label="Dyna-Q+ (standard)")
plt.plot(cumsum(r_action_only), label="Action-selection bonus only")
plt.axvline(1000, color="k", linestyle="--", label="Shortcut Opens")
plt.legend()
plt.title("Exercise 8.4 â€” Shortcut Maze (Figure 8.5)")
plt.xlabel("Steps")
plt.ylabel("Cumulative Reward")
plt.show()
